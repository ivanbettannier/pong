{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from PongEnv import PongEnv\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install nnfigs inf581\".split(), stdout=subprocess.PIPE)\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "    from inf581 import *\n",
    "\n",
    "from inf581.lab7 import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "env = PongEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = env.observation_space.shape\n",
    "action_space = env.action_space.n\n",
    "EPISODES = 500\n",
    "LR = 1e-3\n",
    "MEM_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.99999\n",
    "EXPLORATION_MIN = 0.001\n",
    "sync_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.action_space = action_space\n",
    "        self.input_shape = observation_space\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        print(self.input_shape)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 6, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 6, kernel_size = 5)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 6, out_channels = 6, kernel_size = 5)\n",
    "        self.linear1 = nn.Linear(2268,1024)\n",
    "        self.linear2 = nn.Linear(1024,512)\n",
    "        self.linear3 = nn.Linear(512,self.action_space)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.permute((0,3,1,2))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size = 3)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size = 3)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, kernel_size = 3)\n",
    "        x = self.flatten(x)\n",
    "    \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=MEM_SIZE)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "        state1_batch = torch.stack([s1 for (s1,a,r,s2,d) in minibatch])\n",
    "        action_batch = torch.tensor([a for (s1,a,r,s2,d) in minibatch])\n",
    "        reward_batch = torch.tensor([r for (s1,a,r,s2,d) in minibatch])\n",
    "        state2_batch = torch.stack([s2 for (s1,a,r,s2,d) in minibatch])\n",
    "        done_batch = torch.tensor([d for (s1,a,r,s2,d) in minibatch])\n",
    "\n",
    "        return (state1_batch, action_batch, reward_batch, state2_batch, done_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.replay = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network().to(device)\n",
    "        self.network2 = copy.deepcopy(self.network) #A\n",
    "        self.network2.load_state_dict(self.network.state_dict())\n",
    "        self.prev_action = 0\n",
    "        self.time_prev_action = 5\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.time_prev_action +=1\n",
    "        if self.time_prev_action < 5:\n",
    "            return self.prev_action\n",
    "        self.time_prev_action = 0\n",
    "        if random.random() < self.exploration_rate:\n",
    "            action = env.action_space.sample()\n",
    "            self.prev_action = action\n",
    "            return action\n",
    "\n",
    "        # Convert observation to PyTorch Tensor\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        \n",
    "        q_values = self.network(state).to(device)\n",
    "\n",
    "        # Choose the action to play\n",
    "        action = torch.argmax(q_values).item()\n",
    "        self.prev_action = action\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay.memory)< BATCH_SIZE:\n",
    "            return 0\n",
    "\n",
    "\n",
    "        # Sample minibatch s1, a1, r1, s1', done_1, ... , sn, an, rn, sn', done_n\n",
    "        state1_batch, action_batch, reward_batch, state2_batch, done_batch = self.replay.sample()\n",
    "        state1_batch = state1_batch.to(device)\n",
    "        action_batch = action_batch.to(device)\n",
    "        reward_batch = reward_batch.to(device)\n",
    "        state2_batch = state2_batch.to(device)\n",
    "        done_batch = done_batch.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Q values\n",
    "        q_values = self.network(state1_batch).squeeze()\n",
    "        q_values = q_values.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute next Q values\n",
    "            next_q_values = self.network2(state2_batch).squeeze()\n",
    "            next_q_values = next_q_values.to(device)\n",
    "\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        predicted_value_of_now = q_values[batch_indices, action_batch].to(device)\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0].to(device)\n",
    "\n",
    "        # Compute the q_target\n",
    "        q_target = reward_batch + GAMMA * predicted_value_of_future * (1-(done_batch).long())\n",
    "        q_target = q_target.to(device)\n",
    "\n",
    "        # Compute the loss (c.f. self.network.loss())\n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
    "\n",
    "\n",
    "        # Complute ð›Q\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "        return loss.item()\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 300, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75520014dd4f4c48b9a74f7269a2c236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r6/35j764s5093883lprz06yj680000gn/T/ipykernel_47091/2866714568.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state).float()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r6/35j764s5093883lprz06yj680000gn/T/ipykernel_47091/2866714568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/r6/35j764s5093883lprz06yj680000gn/T/ipykernel_47091/1226465723.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Complute ð›Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQN()\n",
    "EPISODES = 2000\n",
    "env = PongEnv()\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "gif_frequency = 200\n",
    "j=0\n",
    "loss_values = []\n",
    "for i in tqdm(range(1, EPISODES)):\n",
    "    k=0\n",
    "    loss=0\n",
    "    state,_ = env.reset()\n",
    "    RenderWrapper.register(env, force_gif=True)\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        j+=1\n",
    "        k+=1\n",
    "        action = agent.choose_action(state)\n",
    "        if i % gif_frequency == 0:\n",
    "            env.render_wrapper.render()\n",
    "        state_, reward, done, truncated,info = env.step(action)\n",
    "        state = torch.tensor(state).float()\n",
    "        state_ = torch.tensor(state_).float()\n",
    "\n",
    "        exp = (state.to(device), action, reward, state_.to(device), done)\n",
    "        agent.replay.add(exp)\n",
    "        loss += agent.learn()\n",
    "\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if j % sync_freq == 0:\n",
    "            agent.network2.load_state_dict(agent.network.state_dict())\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            \n",
    "          \n",
    "            if i % gif_frequency == 0:\n",
    "                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "                display(env.render_wrapper.make_gif(\"rl3_ex1left\"))\n",
    "            break\n",
    "        loss_values.append(loss/k)\n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)\n",
    "\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()\n",
    "plt.plot(episode_number,loss_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 300, 3)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 796, 446]             456\n",
      "            Conv2d-2          [-1, 6, 261, 144]             906\n",
      "            Conv2d-3            [-1, 6, 83, 44]             906\n",
      "           Flatten-4                 [-1, 2268]               0\n",
      "            Linear-5                 [-1, 1024]       2,323,456\n",
      "            Linear-6                  [-1, 512]         524,800\n",
      "            Linear-7                    [-1, 2]           1,026\n",
      "================================================================\n",
      "Total params: 2,851,550\n",
      "Trainable params: 2,851,550\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.12\n",
      "Forward/backward pass size (MB): 18.17\n",
      "Params size (MB): 10.88\n",
      "Estimated Total Size (MB): 33.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Network().to(device)\n",
    "summary(model,(800,450,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-ram-v5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space = env.observation_space.shape\n",
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
