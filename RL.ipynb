{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from PongEnv import PongEnv\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install nnfigs inf581\".split(), stdout=subprocess.PIPE)\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "    from inf581 import *\n",
    "\n",
    "from inf581.lab7 import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "env = PongEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = env.observation_space.shape\n",
    "action_space = env.action_space.n\n",
    "EPISODES = 500\n",
    "LR = 1e-3\n",
    "MEM_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.99999\n",
    "EXPLORATION_MIN = 0.001\n",
    "sync_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.action_space = action_space\n",
    "        self.input_shape = observation_space\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        print(self.input_shape)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 6, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 6, kernel_size = 5)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 6, out_channels = 6, kernel_size = 5)\n",
    "        self.linear1 = nn.Linear(2268,256)\n",
    "        self.linear2 = nn.Linear(256,128)\n",
    "        self.linear3 = nn.Linear(128,self.action_space)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.permute((0,3,1,2))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size = 3)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size = 3)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, kernel_size = 3)\n",
    "        x = self.flatten(x)\n",
    "    \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=MEM_SIZE)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "        state1_batch = torch.stack([s1 for (s1,a,r,s2,d) in minibatch])\n",
    "        action_batch = torch.tensor([a for (s1,a,r,s2,d) in minibatch])\n",
    "        reward_batch = torch.tensor([r for (s1,a,r,s2,d) in minibatch])\n",
    "        state2_batch = torch.stack([s2 for (s1,a,r,s2,d) in minibatch])\n",
    "        done_batch = torch.tensor([d for (s1,a,r,s2,d) in minibatch])\n",
    "\n",
    "        return (state1_batch, action_batch, reward_batch, state2_batch, done_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.replay = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network().to(device)\n",
    "        self.network2 = copy.deepcopy(self.network) #A\n",
    "        self.network2.load_state_dict(self.network.state_dict())\n",
    "        self.prev_action = 0\n",
    "        self.time_prev_action = 5\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.time_prev_action +=1\n",
    "        if self.time_prev_action < 5:\n",
    "            return self.prev_action\n",
    "        self.time_prev_action = 0\n",
    "        if random.random() < self.exploration_rate:\n",
    "            action = env.action_space.sample()\n",
    "            self.prev_action = action\n",
    "            return action\n",
    "\n",
    "        # Convert observation to PyTorch Tensor\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        \n",
    "        q_values = self.network(state).to(device)\n",
    "\n",
    "        # Choose the action to play\n",
    "        action = torch.argmax(q_values).item()\n",
    "        self.prev_action = action\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay.memory)< BATCH_SIZE:\n",
    "            return 0\n",
    "\n",
    "\n",
    "        # Sample minibatch s1, a1, r1, s1', done_1, ... , sn, an, rn, sn', done_n\n",
    "        state1_batch, action_batch, reward_batch, state2_batch, done_batch = self.replay.sample()\n",
    "        state1_batch = state1_batch.to(device)\n",
    "        action_batch = action_batch.to(device)\n",
    "        reward_batch = reward_batch.to(device)\n",
    "        state2_batch = state2_batch.to(device)\n",
    "        done_batch = done_batch.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Q values\n",
    "        q_values = self.network(state1_batch).squeeze()\n",
    "        q_values = q_values.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute next Q values\n",
    "            next_q_values = self.network2(state2_batch).squeeze()\n",
    "            next_q_values = next_q_values.to(device)\n",
    "\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        predicted_value_of_now = q_values[batch_indices, action_batch].to(device)\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0].to(device)\n",
    "\n",
    "        # Compute the q_target\n",
    "        q_target = reward_batch + GAMMA * predicted_value_of_future * (1-(done_batch).long())\n",
    "        q_target = q_target.to(device)\n",
    "\n",
    "        # Compute the loss (c.f. self.network.loss())\n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
    "\n",
    "\n",
    "        # Complute 𝛁Q\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "        return loss.item()\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 300, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d4a8eb8aa6473ea85ef41b21b276af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_897173/2224676486.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state).float()\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 262.00 MiB. GPU 0 has a total capacty of 7.77 GiB of which 12.94 MiB is free. Including non-PyTorch memory, this process has 7.65 GiB memory in use. Of the allocated memory 7.11 GiB is allocated by PyTorch, and 402.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m exp \u001b[38;5;241m=\u001b[39m (state\u001b[38;5;241m.\u001b[39mto(device), action, reward, state_\u001b[38;5;241m.\u001b[39mto(device), done)\n\u001b[1;32m     29\u001b[0m agent\u001b[38;5;241m.\u001b[39mreplay\u001b[38;5;241m.\u001b[39madd(exp)\n\u001b[0;32m---> 30\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m state \u001b[38;5;241m=\u001b[39m state_\n\u001b[1;32m     33\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[6], line 55\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m q_values \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Compute next Q values\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate2_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     56\u001b[0m     next_q_values \u001b[38;5;241m=\u001b[39m next_q_values\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(BATCH_SIZE, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n",
      "File \u001b[0;32m/Data/ubc/miniconda3/envs/tdlog/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data/ubc/miniconda3/envs/tdlog/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, kernel_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n",
      "File \u001b[0;32m/Data/ubc/miniconda3/envs/tdlog/lib/python3.11/site-packages/torch/nn/functional.py:1471\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 262.00 MiB. GPU 0 has a total capacty of 7.77 GiB of which 12.94 MiB is free. Including non-PyTorch memory, this process has 7.65 GiB memory in use. Of the allocated memory 7.11 GiB is allocated by PyTorch, and 402.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "agent = DQN()\n",
    "EPISODES = 2000\n",
    "env = PongEnv()\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "gif_frequency = 1\n",
    "j=0\n",
    "loss_values = []\n",
    "for i in tqdm(range(1, EPISODES)):\n",
    "    k=0\n",
    "    loss=0\n",
    "    state,_ = env.reset()\n",
    "    RenderWrapper.register(env, force_gif=True)\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        j+=1\n",
    "        k+=1\n",
    "        action = agent.choose_action(state)\n",
    "        if i % gif_frequency == 0:\n",
    "            env.render_wrapper.render()\n",
    "        state_, reward, done, truncated,info = env.step(action)\n",
    "        state = torch.tensor(state).float()\n",
    "        state_ = torch.tensor(state_).float()\n",
    "\n",
    "        exp = (state.to(device), action, reward, state_.to(device), done)\n",
    "        agent.replay.add(exp)\n",
    "        loss += agent.learn()\n",
    "\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if j % sync_freq == 0:\n",
    "            agent.network2.load_state_dict(agent.network.state_dict())\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            \n",
    "          \n",
    "            if i % gif_frequency == 0:\n",
    "                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "                display(env.render_wrapper.make_gif(\"rl3_ex1left\"))\n",
    "            break\n",
    "        loss_values.append(loss/k)\n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)\n",
    "\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()\n",
    "plt.plot(episode_number,loss_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network().to(device)\n",
    "summary(model,(800,450,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-ram-v5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = env.observation_space.shape\n",
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
