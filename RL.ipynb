{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from PongEnv import PongEnv\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install nnfigs inf581\".split(), stdout=subprocess.PIPE)\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "    from inf581 import *\n",
    "\n",
    "from inf581.lab7 import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "env = PongEnv(render_mode='rgb_array')\n",
    "observation_space = env.observation_space.shape\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observation_space = env.observation_space.shape\n",
    "#action_space = env.action_space.n\n",
    "EPISODES = 500\n",
    "LR = 1e-3\n",
    "MEM_SIZE = 2\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.99999\n",
    "EXPLORATION_MIN = 0.001\n",
    "sync_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.action_space = action_space\n",
    "        self.input_shape = observation_space\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        print(self.input_shape)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 6, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 6, kernel_size = 5)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 6, out_channels = 6, kernel_size = 5)\n",
    "        self.linear1 = nn.Linear(2268,128)\n",
    "        self.linear2 = nn.Linear(128,128)\n",
    "        self.linear3 = nn.Linear(128,self.action_space)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.permute((0,3,1,2))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size = 3)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size = 3)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, kernel_size = 3)\n",
    "        x = self.flatten(x)\n",
    "    \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=MEM_SIZE)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "        state1_batch = torch.stack([s1 for (s1,a,r,s2,d) in minibatch])\n",
    "        action_batch = torch.tensor([a for (s1,a,r,s2,d) in minibatch])\n",
    "        reward_batch = torch.tensor([r for (s1,a,r,s2,d) in minibatch])\n",
    "        state2_batch = torch.stack([s2 for (s1,a,r,s2,d) in minibatch])\n",
    "        done_batch = torch.tensor([d for (s1,a,r,s2,d) in minibatch])\n",
    "\n",
    "        return (state1_batch, action_batch, reward_batch, state2_batch, done_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.replay = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network().to(device)\n",
    "        self.network2 = copy.deepcopy(self.network) #A\n",
    "        self.network2.load_state_dict(self.network.state_dict())\n",
    "        self.prev_action = 0\n",
    "        self.time_prev_action = 5\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.time_prev_action +=1\n",
    "        if self.time_prev_action < 5:\n",
    "            return self.prev_action\n",
    "        self.time_prev_action = 0\n",
    "        if random.random() < self.exploration_rate:\n",
    "            action = env.action_space.sample()\n",
    "            self.prev_action = action\n",
    "            return action\n",
    "\n",
    "        # Convert observation to PyTorch Tensor\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        \n",
    "        q_values = self.network(state).to(device)\n",
    "\n",
    "        # Choose the action to play\n",
    "        action = torch.argmax(q_values).item()\n",
    "        self.prev_action = action\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay.memory)< BATCH_SIZE:\n",
    "            return 0\n",
    "\n",
    "\n",
    "        # Sample minibatch s1, a1, r1, s1', done_1, ... , sn, an, rn, sn', done_n\n",
    "        state1_batch, action_batch, reward_batch, state2_batch, done_batch = self.replay.sample()\n",
    "        state1_batch = state1_batch.to(device)\n",
    "        action_batch = action_batch.to(device)\n",
    "        reward_batch = reward_batch.to(device)\n",
    "        state2_batch = state2_batch.to(device)\n",
    "        done_batch = done_batch.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Q values\n",
    "        q_values = self.network(state1_batch).squeeze()\n",
    "        q_values = q_values.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute next Q values\n",
    "            next_q_values = self.network2(state2_batch).squeeze()\n",
    "            next_q_values = next_q_values.to(device)\n",
    "\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        predicted_value_of_now = q_values[batch_indices, action_batch].to(device)\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0].to(device)\n",
    "\n",
    "        # Compute the q_target\n",
    "        q_target = reward_batch + GAMMA * predicted_value_of_future * (1-(done_batch).long())\n",
    "        q_target = q_target.to(device)\n",
    "\n",
    "        # Compute the loss (c.f. self.network.loss())\n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
    "\n",
    "\n",
    "        # Complute 𝛁Q\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "        return loss.item()\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 200, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7de4539be04c7988971b28658fd2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r6/35j764s5093883lprz06yj680000gn/T/ipykernel_13412/3722424645.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 9\n",
      "Best score: 10\n",
      "Best score: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m gif_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     22\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender_wrapper\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 23\u001b[0m state_, reward, done, truncated,info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     25\u001b[0m state_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state_)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/Documents/TDLOG/tdlog-project/PongEnv.py:34\u001b[0m, in \u001b[0;36mPongEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     32\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(collision)\n\u001b[0;32m---> 34\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n",
      "File \u001b[0;32m~/Documents/TDLOG/tdlog-project/PongEnv.py:42\u001b[0m, in \u001b[0;36mPongEnv._get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_obs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     41\u001b[0m     pixarray \u001b[38;5;241m=\u001b[39m  pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpong_game\u001b[38;5;241m.\u001b[39mscreen)\n\u001b[0;32m---> 42\u001b[0m     obs \u001b[38;5;241m=\u001b[39m pixarray\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m pixarray\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQN()\n",
    "EPISODES = 1000\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "gif_frequency = 100\n",
    "j=0\n",
    "loss_values = []\n",
    "for i in tqdm(range(1, EPISODES)):\n",
    "    k=0\n",
    "    loss=0\n",
    "    state,_ = env.reset()\n",
    "    RenderWrapper.register(env, force_gif=True)\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        j+=1\n",
    "        k+=1\n",
    "        action = agent.choose_action(state)\n",
    "        if i % gif_frequency == 0:\n",
    "            env.render_wrapper.render()\n",
    "        state_, reward, done, truncated,info = env.step(action)\n",
    "        state = torch.tensor(state).float()\n",
    "        state_ = torch.tensor(state_).float()\n",
    "\n",
    "        exp = (state.to(device), action, reward, state_.to(device), done)\n",
    "        agent.replay.add(exp)\n",
    "        loss += agent.learn()\n",
    "\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if j % sync_freq == 0:\n",
    "            agent.network2.load_state_dict(agent.network.state_dict())\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "                if best_reward > 2 :\n",
    "                    print(f'Best score: {best_reward}')\n",
    "                    #display(env.render_wrapper.make_gif(\"best_score\"))\n",
    "            average_reward += score \n",
    "            \n",
    "          \n",
    "            if i % gif_frequency == 0:\n",
    "                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "                display(env.render_wrapper.make_gif(\"rl3_ex1left\"))\n",
    "  \n",
    "                \n",
    "            break\n",
    "        loss_values.append(loss/k)\n",
    "        episode_number.append(i)\n",
    "        average_reward_number. append(average_reward/i)\n",
    "    #gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()\n",
    "plt.plot(episode_number,loss_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network().to(device)\n",
    "summary(model,(800,450,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-ram-v5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = env.observation_space.shape\n",
    "observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render_wrapper.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.render_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
